# 描述：

搜索引擎可以通过robots文件可以获知哪些页面可以爬取，哪些页面不可以爬取。Robots协议是网站国际互联网界通行的道德规范，其目的是保护网站数据和敏感信息、确保用户个人信息和隐私不被侵犯，如果robots.txt文件编辑的太过详细，反而会泄露网站的敏感目录或者文件，比如网站后台路径，从而得知其使用的系统类型，从而有针对性地进行利用。

# 检测方法：

> 1. 检测形式多样，工具爬虫扫描得到敏感文件的路径，从而找到robots文件，
> 2. 手工挖掘，直接在域名后输入/robots.txt进行查看。



https://blog.csdn.net/weixin_39934520/article/details/107419996